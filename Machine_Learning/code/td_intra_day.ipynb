{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "169c05fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import sys\n",
    "# import glob\n",
    "# from datetime import datetime,timedelta,time\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "# import sys\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "# from configs import db_config\n",
    "# from configs.path_config import *\n",
    "# from sqlalchemy import create_engine\n",
    "# from pandas import read_sql_query\n",
    "# from d6tstack.utils import pd_to_psql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8738c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# from pandas.core.common import SettingWithCopyWarning\n",
    "\n",
    "# warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "977d3b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_db_connection(dbname,\n",
    "#                          host,\n",
    "#                          port,\n",
    "#                          user,\n",
    "#                          password):\n",
    "#     \"\"\"\n",
    "#     Establishes connection to database\n",
    "#     :param dbname: database name\n",
    "#     :param host: endpoint\n",
    "#     :param port: port\n",
    "#     :param user: username\n",
    "#     :param password: password\n",
    "#     \"\"\"\n",
    "#     connection_string = f'postgresql://{user}:{password}@{host}:{port}/{dbname}'\n",
    "#     return create_engine(connection_string)\n",
    "\n",
    "\n",
    "# class SiteDataExtractor:\n",
    "\n",
    "#     def __init__(self, \n",
    "#                  db_connection, \n",
    "#                  table_name,\n",
    "#                  schema_name,\n",
    "#                  site_name,\n",
    "#                  today_date,\n",
    "#                  site_date_label='timestamp',\n",
    "#                  site_column_label='site_name',\n",
    "#                  eng='pandas'):\n",
    "#         \"\"\"\n",
    "#         General purpose class to extract data for a specific site from the schema.table address of the db connection\n",
    "#         :param db_connection: sql alchemy db connection\n",
    "#         :param table_name: str, table name in the db to extract from\n",
    "#         :param schema_name: str, schema name in the db to extract from\n",
    "#         :param site_column_label: str, site identifier column label i.e. name of column containing the site names\n",
    "#         :param eng: str, ['connectorx', 'pandas']. Defaults to pandas. Can show time improvements in production.\n",
    "#         \"\"\"\n",
    "#         self.db_connection = db_connection\n",
    "#         self.table_name = table_name\n",
    "#         self.schema_name = schema_name\n",
    "#         self.site_name = site_name\n",
    "#         self.today_date = today_date\n",
    "#         self.site_column_label = site_column_label\n",
    "#         self.site_date_label = site_date_label\n",
    "#         self.eng = eng\n",
    "#         self.db_str = 'postgresql://admin123:tensor123@tensordb1.cn6gzof6sqbw.us-east-2.rds.amazonaws.com:5432/postgres'\n",
    "\n",
    "#     def parse_query(self, query):\n",
    "#         \"\"\" parses sql query via the desired engine \"\"\"\n",
    "#         if self.eng.lower() == 'pandas':\n",
    "#             return pd.read_sql_query(sql=query, con=self.db_connection)\n",
    "#         elif self.eng.lower() == 'connectorx':\n",
    "#             return cx.read_sql(conn=self.db_str, query=query)\n",
    "#         else:\n",
    "#             raise NotImplementedError(f\"Only 'pandas' and 'connectorx' are valid choices for eng in __init__ call. {self.eng} was provided.\")\n",
    "\n",
    "#     def read_data(self):\n",
    "#         \"\"\" reads data for a specific site from database \"\"\"\n",
    "#         query = f\"select * from {self.schema_name}.{self.table_name}\"                 f\" where {self.site_column_label} = '{self.site_name}' and {self.site_date_label}::date = '{self.today_date}'\"\n",
    "#         return self.parse_query(query=query)\n",
    "\n",
    "# def utc_to_ist(data_frame, time_col='timestamp'):\n",
    "#     temp_data = data_frame.copy()\n",
    "#     temp_data[time_col] = pd.to_datetime(temp_data[time_col], utc=True)\n",
    "#     return temp_data.set_index(time_col).tz_convert('Asia/Kolkata').reset_index()\n",
    "\n",
    "# def remove_timezone(data_frame, time_col='timestamp'):\n",
    "#     temp_data = data_frame.copy()\n",
    "#     temp_data[time_col] = temp_data[time_col].dt.tz_localize(None)\n",
    "#     return temp_data\n",
    "\n",
    "# def extract_avail_timeseries_for_variable(data_frame, time_col, variable='ct'):\n",
    "#     return data_frame[[time_col, variable]].set_index(time_col)\n",
    "# def pre_process_satellite_data(data_frame, time_col='timestamp', variable='ct'):\n",
    "#     output = (data_frame\n",
    "#               .copy()\n",
    "#               .pipe(utc_to_ist, time_col)\n",
    "#               .pipe(remove_timezone, time_col)\n",
    "#               .pipe(extract_avail_timeseries_for_variable, time_col, variable)\n",
    "#               .sort_index())\n",
    "#     return output\n",
    "\n",
    "# def roundTime(dt, roundTo=15*60):\n",
    "#    \"\"\"\n",
    "#    Round a datetime object to any time lapse in seconds\n",
    "#    dt : datetime.datetime object, default now.\n",
    "#    roundTo : Closest number of seconds to round to, default 1 minute.\n",
    "#    \"\"\"\n",
    "#    if dt != None :\n",
    "#         dt = datetime.now()\n",
    "#         seconds = (dt.replace(tzinfo=None) - dt.min).seconds\n",
    "#         rounding = (seconds+roundTo/2) // roundTo * roundTo\n",
    "#         return dt + timedelta(0,rounding-seconds,-dt.microsecond)\n",
    "\n",
    "# def day_ahead(db_connection,\n",
    "#               table,\n",
    "#               schema,\n",
    "#               date,\n",
    "#               site):\n",
    "    \n",
    "#     day_ahead_data = SiteDataExtractor(db_connection=db_connection, \n",
    "#                                        table_name=table,\n",
    "#                                        schema_name=schema, \n",
    "#                                        today_date = str(date),\n",
    "#                                        site_name=site).read_data() \n",
    "#     if day_ahead_data.shape[0]>0:\n",
    "        \n",
    "#         day_ahead_data['timestamp'] = day_ahead_data['timestamp'].dt.round('15min')\n",
    "#         day_ahead_data = day_ahead_data.drop_duplicates(subset=['timestamp','site_name'], keep='last').set_index('timestamp')\n",
    "#         day_ahead_data = day_ahead_data[['swdown_wpm2','site_name']]\n",
    "#         day_ahead_data = day_ahead_data.rename(columns={'swdown_wpm2':'ghi_predicted(w/m2)'})\n",
    "#         day_ahead_data['forecast_method'] = len(day_ahead_data.index)*['day_ahead']\n",
    "#         day_ahead_data['time'] = day_ahead_data.index.time\n",
    "#         print(f\"fetched day_ahead for {site} with {day_ahead_data.shape[0]} rows\")\n",
    "#         return day_ahead_data.sort_index()\n",
    "#     else:\n",
    "#         return pd.DataFrame()\n",
    "    \n",
    "    \n",
    "# def clearsky(db_connection,\n",
    "#              table,\n",
    "#              schema,\n",
    "#              date,\n",
    "#              site,\n",
    "#              date_label='times'):\n",
    "    \n",
    "#     clearsky_data = SiteDataExtractor(db_connection=db_connection, \n",
    "#                                        table_name= table,\n",
    "#                                        schema_name=schema,\n",
    "#                                        site_date_label=date_label,\n",
    "#                                        today_date = str(date),\n",
    "#                                        site_name=site).read_data() \n",
    "#     if clearsky_data.shape[0]>0:\n",
    "#         clearsky_data['times'] = clearsky_data[date_label].dt.round('15min')\n",
    "#         clearsky_data = clearsky_data.drop_duplicates(subset=[date_label,'site_name'], keep='last').set_index('times')\n",
    "#         clearsky_data = clearsky_data[['swdnbc','site_name']]\n",
    "#         clearsky_data = clearsky_data.reset_index().rename(columns={'swdnbc':'cs',\n",
    "#                                                                     'times':'timestamp'}).set_index('timestamp')\n",
    "#         print(f\"fetched clearsky for {site} with {clearsky_data.shape[0]} rows\")\n",
    "#         return clearsky_data.sort_index()\n",
    "#     else:\n",
    "#         return pd.DataFrame()\n",
    "    \n",
    "# def satellite_ct(db_connection,\n",
    "#                  table,\n",
    "#                  schema,\n",
    "#                  timestamp,\n",
    "#                  date,\n",
    "#                  site,\n",
    "#                  satellite_time_col = 'timestamp',\n",
    "#                  satellite_ct_col = 'ct'):\n",
    "    \n",
    "#     date_utc = (ts-timedelta(hours=5.50)).date()\n",
    "    \n",
    "#     satellite_data = SiteDataExtractor(db_connection=db_connection, \n",
    "#                                        table_name=table,\n",
    "#                                        schema_name=schema, \n",
    "#                                        today_date = str(date_utc),\n",
    "#                                        site_name=site).read_data()\n",
    "#     if (satellite_data.shape[0]>0):\n",
    "#         satellite_ct_series = pre_process_satellite_data(data_frame=satellite_data, \n",
    "#                                                          time_col= satellite_time_col,\n",
    "#                                                          variable=satellite_ct_col)\n",
    "#         satellite_ct_series = satellite_ct_series.reset_index().drop_duplicates(subset=satellite_time_col, keep='last').set_index(satellite_time_col)\n",
    "#         satellite_ct_series = satellite_ct_series.dropna(subset=satellite_ct_col)\n",
    "#         print(f\"Fetched satellite data for {site} with {satellite_ct_series.shape[0]} rows\")\n",
    "        \n",
    "#         return satellite_ct_series.shift(-1).sort_index()\n",
    "#     else:\n",
    "#         return pd.DataFrame()\n",
    "    \n",
    "\n",
    "\n",
    "# def slice_forecast(df,start,end):\n",
    "#     temp_df = df.copy()\n",
    "# #     temp['time'] = temp.index.time\n",
    "#     temp_df = temp_df[(temp_df['time']>=start) &\n",
    "#                         (temp_df['time']<=end)]\n",
    "#     return temp_df\n",
    "\n",
    "# def real_data(db_connection,\n",
    "#               table,\n",
    "#               schema,\n",
    "#               date,\n",
    "#               site,\n",
    "#               time_col = 'timestamp',\n",
    "#               rad_col = 'ghi(w/m2)',\n",
    "#               power_col = 'power(kw)'):\n",
    "    \n",
    "#     real_data = SiteDataExtractor(db_connection=db_connection, \n",
    "#                                        table_name= table,\n",
    "#                                        schema_name= schema, \n",
    "#                                        today_date = str(date),\n",
    "#                                        site_name=site).read_data()\n",
    "#     if (real_data.shape[0]>0):\n",
    "#         real_data = real_data[[time_col,rad_col]].set_index(time_col)\n",
    "#         real_data.index = pd.to_datetime(real_data.index)\n",
    "#         real_data['time'] = real_data.index.time\n",
    "#         print(f\"real data is fetched for {site} with {real_data.shape[0]} rows\")\n",
    "#         return real_data\n",
    "#     else:\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "# def read_sample_table_from_DB(schema,table,con):\n",
    "#     query = f\"select * from {schema}.{table} limit 1 \"\n",
    "#     df = pd.read_sql_query(sql=query,con=con, index_col='timestamp')\n",
    "#     return df.columns\n",
    "\n",
    "# def fill_values_to_zero_in_night(df,\n",
    "#                                  sunrise = time(5,30),\n",
    "#                                  sunset = time(17,30),\n",
    "#                                  target_col= 'ghi(w/m2)'):\n",
    "    \n",
    "#     \"\"\"\n",
    "#     time_series index is datetime index\n",
    "#     \"\"\"\n",
    "#     df[target_col] = df[target_col].apply(lambda x: 0 if x < 0 else x)\n",
    "#     for i in df.index:\n",
    "#         if ((sunset < (i.time())) | ((i.time()) < sunrise)):\n",
    "#             #         print(i)\n",
    "#             df.at[i, target_col] = 0\n",
    "#     return df\n",
    "\n",
    "# def clearsky_roll_and_shift(clearsky_df,\n",
    "#                            window=3,\n",
    "#                            target_col = 'cs'):\n",
    "#     \"\"\"\n",
    "#     clearsky_df index is datetime index.\n",
    "#     window is number of timeblock for which dataframe have to roll\n",
    "#     \"\"\"\n",
    "#     temp_df = clearsky_df.copy()\n",
    "#     for ts in tqdm(temp_df.index):\n",
    "#         ts_date = ts.date()\n",
    "#         ts_time = ts.time()\n",
    "#         last_n_block_ago_timestamp = ts - pd.Timedelta(minutes=window*15)\n",
    "#         req_cs_series = temp_df.loc[(temp_df.index >= last_n_block_ago_timestamp) & \n",
    "#                                      (temp_df.index <= ts)][target_col]\n",
    "#         req_cs = req_cs_series.mean()\n",
    "#         temp_df.loc[ts,'cs_rolled'] = req_cs\n",
    "    \n",
    "#     temp_df['cs_rolled'] = temp_df['cs_rolled'].shift(-2)\n",
    "#     temp_df = fill_values_to_zero_in_night(df=temp_df,\n",
    "#                                            sunrise=time(5,0),\n",
    "#                                            sunset=time(18,0),\n",
    "#                                            target_col='cs_rolled')\n",
    "#     return temp_df.drop(columns=target_col).sort_index()\n",
    "\n",
    "# def ct_postprocessing(satellite_df,\n",
    "#                      window=3,\n",
    "#                      target_col = 'ci'):\n",
    "#     \"\"\"\n",
    "#     satellite_df index is datetime index.\n",
    "#     cloud index would be rolled for previous 3 timeblocks\n",
    "#     window is number of timeblock for which cloud index have to roll\n",
    "#     \"\"\"\n",
    "#     temp_df = satellite_df.copy()\n",
    "#     for ts in tqdm(temp_df.index):\n",
    "#         ts_date = ts.date()\n",
    "#         ts_time = ts.time()\n",
    "#         last_n_block_ago_timestamp = ts - pd.Timedelta(minutes=window*15)\n",
    "#         req_ci_series = temp_df.loc[(temp_df.index >= last_n_block_ago_timestamp) & \n",
    "#                                      (temp_df.index <= ts)][target_col]\n",
    "#         ci_value = req_ci_series.mean()\n",
    "#         temp_df.loc[ts,'ci_past_mean'] = ci_value\n",
    "#     return temp_df.drop(columns='ci').sort_index()\n",
    "        \n",
    "# def nowcasting(satellite_df,\n",
    "#                clearsky_df,\n",
    "#                site,\n",
    "#                date,\n",
    "#                clearsky_col = 'cs_rolled',\n",
    "#                cloud_index_column = 'ci_past_mean',\n",
    "#                ct_column = 'ct'):\n",
    "#     satellite_df = satellite_df.dropna(subset=ct_column)\n",
    "#     nowcasting_df = pd.DataFrame()\n",
    "#     nowcasting_df.index = pd.date_range(start=date,periods=96,freq='15min' )\n",
    "#     for ts in tqdm(nowcasting_df.index):\n",
    "#         try:\n",
    "#             req_cs = clearsky_df.loc[ts,clearsky_col]\n",
    "#             req_ci = satellite_df.loc[ts,cloud_index_column]\n",
    "#             nowcasting_df.loc[ts,'ghi_predicted(w/m2)'] = (1-req_ci)*req_cs\n",
    "#             nowcasting_df.loc[ts,'site_name'] = site\n",
    "#             nowcasting_df.loc[ts,'forecast_method'] = 'sat_forecast'\n",
    "#             nowcasting_df.loc[ts,'time'] = ts.time()\n",
    "#         except:\n",
    "#             print(f\"clearsky value or ci value is missing for timestamp {ts}\")\n",
    "#     return nowcasting_df.sort_index()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "449c192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcs import *\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e1197d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening DB connection\n"
     ]
    }
   ],
   "source": [
    "db_connection = get_sql_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e13a64d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2023, 3, 24, 10, 22, 3, 137686)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5645a1c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:27:03.866118\n",
      "intra day script current time is 2023-03-24 07:00:00 \n",
      "Opening DB connection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nowcasting is in progress for SPP1 site\n",
      "Opening DB connection\n",
      "fetched day_ahead for SPP1 with 96 rows\n",
      "fetched clearsky for SPP1 with 96 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████| 96/96 [00:00<00:00, 1899.32it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched satellite data for SPP1 with 61 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████| 61/61 [00:00<00:00, 1954.46it/s]\u001b[A\n",
      "\n",
      "100%|█████████████████████████████████████████| 96/96 [00:00<00:00, 2911.09it/s]\u001b[A\n",
      "\n",
      "100%|████████████████████████████████████████| 96/96 [00:00<00:00, 40483.93it/s]\u001b[A\n",
      "\n",
      "100%|████████████████████████████████████████| 96/96 [00:00<00:00, 36327.43it/s]\u001b[A\n",
      "\n",
      "100%|████████████████████████████████████████| 96/96 [00:00<00:00, 34741.43it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening DB connection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                   | 0/575 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|████████████▊                          | 188/575 [00:00<00:00, 1877.68it/s]\u001b[A\n",
      " 66%|█████████████████████████▌             | 377/575 [00:00<00:00, 1884.38it/s]\u001b[A\n",
      "100%|███████████████████████████████████████| 575/575 [00:00<00:00, 1900.36it/s]\u001b[A\n",
      "\n",
      "100%|█████████████████████████████████████████| 96/96 [00:00<00:00, 6311.48it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving intra day predictions for SPP1 site\n",
      "writing log file for SPP1 site\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████| 96/96 [00:00<00:00, 7641.06it/s]\u001b[A\n",
      "\n",
      "100%|█████████████████████████████████████████| 96/96 [00:00<00:00, 3598.43it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB intra_day data deleted for 2023-03-24 date and SPP1 site \n",
      "Intra day data uploaded to DB for date 2023-03-24 and SPP1 site \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                     | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      " 17%|███████▌                                     | 1/6 [00:01<00:05,  1.01s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB week_ahead_api data deleted for 2023-03-27 date and SPP1 site \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|███████████████                              | 2/6 [00:01<00:03,  1.04it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB week_ahead_api data deleted for 2023-03-25 date and SPP1 site \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|██████████████████████▌                      | 3/6 [00:03<00:03,  1.04s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB week_ahead_api data deleted for 2023-03-26 date and SPP1 site \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|██████████████████████████████               | 4/6 [00:04<00:02,  1.13s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB week_ahead_api data deleted for 2023-03-28 date and SPP1 site \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 83%|█████████████████████████████████████▌       | 5/6 [00:05<00:01,  1.04s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB week_ahead_api data deleted for 2023-03-29 date and SPP1 site \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:06<00:00,  1.03s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB week_ahead_api data deleted for 2023-03-24 date and SPP1 site \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|█████████                                    | 1/5 [00:27<01:48, 27.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week_ahead data uploaded to DB for date 2023-03-24 and SPP1 site \n",
      "Nowcasting is in progress for SPP2 site\n",
      "Opening DB connection\n",
      "fetched day_ahead for SPP2 with 96 rows\n",
      "fetched clearsky for SPP2 with 96 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████| 96/96 [00:00<00:00, 1924.03it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched satellite data for SPP2 with 61 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████| 61/61 [00:00<00:00, 1969.93it/s]\u001b[A\n",
      "\n",
      "100%|█████████████████████████████████████████| 96/96 [00:00<00:00, 2995.57it/s]\u001b[A\n",
      "\n",
      "100%|████████████████████████████████████████| 96/96 [00:00<00:00, 32861.60it/s]\u001b[A\n",
      "\n",
      "100%|████████████████████████████████████████| 96/96 [00:00<00:00, 36741.78it/s]\u001b[A\n",
      "\n",
      "100%|████████████████████████████████████████| 96/96 [00:00<00:00, 25783.00it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening DB connection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                   | 0/575 [00:00<?, ?it/s]\u001b[A\n",
      " 35%|█████████████▋                         | 201/575 [00:00<00:00, 2001.10it/s]\u001b[A\n",
      "100%|███████████████████████████████████████| 575/575 [00:00<00:00, 2010.57it/s]\u001b[A\n",
      "\n",
      "100%|█████████████████████████████████████████| 96/96 [00:00<00:00, 7736.19it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving intra day predictions for SPP2 site\n",
      "writing log file for SPP2 site\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████| 96/96 [00:00<00:00, 7371.77it/s]\u001b[A\n",
      "\n",
      "100%|█████████████████████████████████████████| 96/96 [00:00<00:00, 3619.55it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB intra_day data deleted for 2023-03-24 date and SPP2 site \n",
      "Intra day data uploaded to DB for date 2023-03-24 and SPP2 site \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                     | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      " 17%|███████▌                                     | 1/6 [00:00<00:04,  1.08it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB week_ahead_api data deleted for 2023-03-27 date and SPP2 site \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|███████████████                              | 2/6 [00:01<00:03,  1.11it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB week_ahead_api data deleted for 2023-03-25 date and SPP2 site \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|██████████████████████▌                      | 3/6 [00:02<00:02,  1.02it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB week_ahead_api data deleted for 2023-03-26 date and SPP2 site \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|██████████████████████████████               | 4/6 [00:03<00:01,  1.03it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB week_ahead_api data deleted for 2023-03-28 date and SPP2 site \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 83%|█████████████████████████████████████▌       | 5/6 [00:04<00:00,  1.03it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB week_ahead_api data deleted for 2023-03-29 date and SPP2 site \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:05<00:00,  1.05it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB week_ahead_api data deleted for 2023-03-24 date and SPP2 site \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|██████████████████                           | 2/5 [00:49<01:13, 24.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week_ahead data uploaded to DB for date 2023-03-24 and SPP2 site \n",
      "Nowcasting is in progress for SPP3 site\n",
      "Opening DB connection\n",
      "fetched day_ahead for SPP3 with 96 rows\n",
      "fetched clearsky for SPP3 with 96 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████| 96/96 [00:00<00:00, 1913.45it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched satellite data for SPP3 with 61 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████| 61/61 [00:00<00:00, 1928.79it/s]\u001b[A\n",
      "\n",
      "100%|█████████████████████████████████████████| 96/96 [00:00<00:00, 2904.58it/s]\u001b[A\n",
      "\n",
      "100%|████████████████████████████████████████| 96/96 [00:00<00:00, 41777.67it/s]\u001b[A\n",
      "\n",
      "100%|████████████████████████████████████████| 96/96 [00:00<00:00, 39222.01it/s]\u001b[A\n",
      "\n",
      "100%|████████████████████████████████████████| 96/96 [00:00<00:00, 29440.17it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening DB connection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                   | 0/575 [00:00<?, ?it/s]\u001b[A\n",
      " 34%|█████████████▎                         | 197/575 [00:00<00:00, 1967.26it/s]\u001b[A\n",
      "100%|███████████████████████████████████████| 575/575 [00:00<00:00, 1976.46it/s]\u001b[A\n",
      "\n",
      "100%|█████████████████████████████████████████| 96/96 [00:00<00:00, 7487.60it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving intra day predictions for SPP3 site\n",
      "writing log file for SPP3 site\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████| 96/96 [00:00<00:00, 7469.40it/s]\u001b[A\n",
      "\n",
      "100%|█████████████████████████████████████████| 96/96 [00:00<00:00, 3218.39it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB intra_day data deleted for 2023-03-24 date and SPP3 site \n",
      "Intra day data uploaded to DB for date 2023-03-24 and SPP3 site \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                     | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      " 17%|███████▌                                     | 1/6 [00:00<00:04,  1.12it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB week_ahead_api data deleted for 2023-03-27 date and SPP3 site \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|███████████████                              | 2/6 [00:01<00:03,  1.11it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB week_ahead_api data deleted for 2023-03-25 date and SPP3 site \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|██████████████████████▌                      | 3/6 [00:02<00:02,  1.12it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB week_ahead_api data deleted for 2023-03-26 date and SPP3 site \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|██████████████████████████████               | 4/6 [00:03<00:01,  1.08it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB week_ahead_api data deleted for 2023-03-28 date and SPP3 site \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 83%|█████████████████████████████████████▌       | 5/6 [00:04<00:00,  1.04it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB week_ahead_api data deleted for 2023-03-29 date and SPP3 site \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:05<00:00,  1.04it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB week_ahead_api data deleted for 2023-03-24 date and SPP3 site \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|███████████████████████████                  | 3/5 [01:14<00:49, 24.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week_ahead data uploaded to DB for date 2023-03-24 and SPP3 site \n",
      "Nowcasting is in progress for SPP4 site\n",
      "Opening DB connection\n",
      "fetched day_ahead for SPP4 with 96 rows\n",
      "fetched clearsky for SPP4 with 96 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████| 96/96 [00:00<00:00, 1790.45it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched satellite data for SPP4 with 61 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████| 61/61 [00:00<00:00, 2020.54it/s]\u001b[A\n",
      "\n",
      "100%|█████████████████████████████████████████| 96/96 [00:00<00:00, 3075.94it/s]\u001b[A\n",
      "\n",
      "100%|████████████████████████████████████████| 96/96 [00:00<00:00, 40096.91it/s]\u001b[A\n",
      "\n",
      "100%|████████████████████████████████████████| 96/96 [00:00<00:00, 33279.87it/s]\u001b[A\n",
      "\n",
      "100%|████████████████████████████████████████| 96/96 [00:00<00:00, 26634.02it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening DB connection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                   | 0/575 [00:00<?, ?it/s]\u001b[A\n",
      " 34%|█████████████▎                         | 197/575 [00:00<00:00, 1968.70it/s]\u001b[A\n",
      "100%|███████████████████████████████████████| 575/575 [00:00<00:00, 1987.82it/s]\u001b[A\n",
      "\n",
      "100%|█████████████████████████████████████████| 96/96 [00:00<00:00, 8201.51it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving intra day predictions for SPP4 site\n",
      "writing log file for SPP4 site\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████| 96/96 [00:00<00:00, 7854.35it/s]\u001b[A\n",
      "\n",
      "100%|█████████████████████████████████████████| 96/96 [00:00<00:00, 3865.12it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB intra_day data deleted for 2023-03-24 date and SPP4 site \n",
      "Intra day data uploaded to DB for date 2023-03-24 and SPP4 site \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                     | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      " 17%|███████▌                                     | 1/6 [00:00<00:04,  1.13it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB week_ahead_api data deleted for 2023-03-27 date and SPP4 site \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|███████████████                              | 2/6 [00:01<00:03,  1.14it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB week_ahead_api data deleted for 2023-03-25 date and SPP4 site \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|██████████████████████▌                      | 3/6 [00:02<00:02,  1.13it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB week_ahead_api data deleted for 2023-03-26 date and SPP4 site \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|██████████████████████████████               | 4/6 [00:03<00:01,  1.14it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB week_ahead_api data deleted for 2023-03-28 date and SPP4 site \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 83%|█████████████████████████████████████▌       | 5/6 [00:04<00:00,  1.13it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB week_ahead_api data deleted for 2023-03-29 date and SPP4 site \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:05<00:00,  1.13it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB week_ahead_api data deleted for 2023-03-24 date and SPP4 site \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 80%|████████████████████████████████████         | 4/5 [01:34<00:22, 22.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week_ahead data uploaded to DB for date 2023-03-24 and SPP4 site \n",
      "Nowcasting is in progress for SPP5 site\n",
      "Opening DB connection\n",
      "fetched day_ahead for SPP5 with 96 rows\n",
      "fetched clearsky for SPP5 with 96 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████| 96/96 [00:00<00:00, 1883.03it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched satellite data for SPP5 with 61 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████| 61/61 [00:00<00:00, 1942.32it/s]\u001b[A\n",
      "\n",
      "100%|█████████████████████████████████████████| 96/96 [00:00<00:00, 3104.33it/s]\u001b[A\n",
      "\n",
      "100%|████████████████████████████████████████| 96/96 [00:00<00:00, 41999.91it/s]\u001b[A\n",
      "\n",
      "100%|████████████████████████████████████████| 96/96 [00:00<00:00, 34904.06it/s]\u001b[A\n",
      "\n",
      "100%|████████████████████████████████████████| 96/96 [00:00<00:00, 28546.84it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening DB connection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                   | 0/575 [00:00<?, ?it/s]\u001b[A\n",
      " 35%|█████████████▍                         | 199/575 [00:00<00:00, 1988.87it/s]\u001b[A\n",
      "100%|███████████████████████████████████████| 575/575 [00:00<00:00, 1983.54it/s]\u001b[A\n",
      "\n",
      "100%|█████████████████████████████████████████| 96/96 [00:00<00:00, 7509.52it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving intra day predictions for SPP5 site\n",
      "writing log file for SPP5 site\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████| 96/96 [00:00<00:00, 7627.16it/s]\u001b[A\n",
      "\n",
      "100%|█████████████████████████████████████████| 96/96 [00:00<00:00, 3881.59it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB intra_day data deleted for 2023-03-24 date and SPP5 site \n",
      "Intra day data uploaded to DB for date 2023-03-24 and SPP5 site \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                     | 0/6 [00:00<?, ?it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "from funcs import *\n",
    "import pytz\n",
    "print(datetime.now().time())\n",
    "# ts = roundTime(datetime.now(pytz.timezone('Asia/Kolkata')),roundTo=15*60)\n",
    "ts = datetime(2023,3,24,7)\n",
    "print(f\"intra day script current time is {str(ts)} \")\n",
    "\n",
    "prev_forecast_start = time(0,0)\n",
    "prev_forecast_end = (ts).time()\n",
    "\n",
    "sat_forecast_start = (ts+timedelta(hours =0.25)).time()\n",
    "sat_forecast_end = (ts+timedelta(hours=3)).time()\n",
    "\n",
    "day_ahead_forecast_start = (ts+timedelta(hours=3.25)).time()\n",
    "day_ahead_forecast_end = time(23,45)\n",
    "db_connection = get_sql_connection()\n",
    "sample_df_columns = read_sample_table_from_DB(schema=db_config.forecast_schema,\n",
    "                                              table=db_config.forecast_table,\n",
    "                                             con=db_connection)\n",
    "ct_ci_df = pd.read_csv(os.path.join(resource_path,'trained_ci.csv'))\n",
    "ct_flag_dict={}\n",
    "ci_dict = {}\n",
    "for _, r in ct_ci_df.iterrows():\n",
    "    ci_dict[r['CT_Index']]= r['ci_wrf']\n",
    "    ct_flag_dict[r['CT_Index']] =  r['CT_Flag']\n",
    "\n",
    "for site in tqdm(sites):\n",
    "    print(f\"Nowcasting is in progress for {site} site\")\n",
    "    db_connection = get_sql_connection()\n",
    "    intra_day_path = os.path.join(home,'OUTPUT',site)\n",
    "    day_ahead_df = day_ahead(db_connection=db_connection,\n",
    "                              schema=db_config.wrf_schema,\n",
    "                              table = db_config.wrf_view,\n",
    "                              site=site,\n",
    "                              date = ts.date())\n",
    "    clearsky_df = clearsky(db_connection=db_connection,table=db_config.site_clearsky_table, schema=db_config.site_actual_schema,\n",
    "                          date= ts.date(),site=site)\n",
    "    cs_rolled = clearsky_roll_and_shift(clearsky_df=clearsky_df)\n",
    "\n",
    "    satellite_df = satellite_ct(db_connection=db_connection,\n",
    "                               table = db_config.satellite_ct_exim_ip_view,\n",
    "                               schema=db_config.satellite_schema,\n",
    "                               timestamp=ts,\n",
    "                               date=ts.date(),\n",
    "                               site=site)\n",
    "    satellite_df['ci'] = satellite_df['ct'].map(ci_dict)\n",
    "#     satellite_df['ct_flag'] = satellite_df['ct'].map(ct_flag_dict)\n",
    "    satellite_ct_ci_df = ct_postprocessing(satellite_df=satellite_df)\n",
    "    nowcasting_df = nowcasting(satellite_df=satellite_ct_ci_df,\n",
    "                              clearsky_df=cs_rolled,\n",
    "                              site=site,\n",
    "                              date=ts.date())\n",
    "\n",
    "    try:\n",
    "        prev_forecast_file = max(glob.glob(os.path.join(intra_day_path,str(ts.date()))+'/*'), key = os.path.getmtime)\n",
    "        prev_forecast_df = pd.read_csv(prev_forecast_file,\n",
    "                                       index_col='timestamp',\n",
    "                                       parse_dates=True)\n",
    "        prev_forecast_df['forecast_method'] = len(prev_forecast_df.index)*['prev_forecast_file']\n",
    "        prev_forecast_df['time'] = prev_forecast_df.index.time\n",
    "    except:\n",
    "        prev_forecast_df = day_ahead_df\n",
    "\n",
    "\n",
    "    intra_day_first_df = slice_forecast(df = prev_forecast_df,\n",
    "                                        start = prev_forecast_start,\n",
    "                                        end = prev_forecast_end) \n",
    "\n",
    "    intra_day_second_df = slice_forecast(df = nowcasting_df,\n",
    "                                        start = sat_forecast_start,\n",
    "                                        end = sat_forecast_end)\n",
    "    intra_day_third_df = slice_forecast(df = day_ahead_df,\n",
    "                                        start = day_ahead_forecast_start,\n",
    "                                        end = day_ahead_forecast_end) \n",
    "\n",
    "    intra_day_df = pd.concat([intra_day_first_df,\n",
    "                              intra_day_second_df,\n",
    "                              intra_day_third_df], axis=0)\n",
    "\n",
    "\n",
    "    intra_day_df['ghi_predicted(w/m2)'] = intra_day_df['ghi_predicted(w/m2)'].clip(lower=0)\n",
    "    intra_day_df = fill_values_to_zero_in_night(df=intra_day_df,\n",
    "                                                target_col= 'ghi_predicted(w/m2)')\n",
    "    intra_day_df = fill_missing_with_next_and_prev_mean(df=intra_day_df)\n",
    "\n",
    "    #filling missing values to clearsky\n",
    "    for i in tqdm(intra_day_df.index):\n",
    "        if pd.isnull(intra_day_df.loc[i,'ghi_predicted(w/m2)']):\n",
    "            intra_day_df.loc[i,'ghi_predicted(w/m2)'] = cs_rolled.loc[i,'cs_rolled']\n",
    "            intra_day_df.loc[i,'forecast_method'] = 'clearsky_push'\n",
    "    #filling missing values to day ahead\n",
    "    for i in tqdm(intra_day_df.index):\n",
    "        if np.isnan(intra_day_df.loc[i,'ghi_predicted(w/m2)']):\n",
    "            intra_day_df.loc[i,'ghi_predicted(w/m2)'] = day_ahead_df.loc[i,'ghi_predicted(w/m2)']\n",
    "            intra_day_df.loc[i,'forecast_method'] = 'day_ahead_push'\n",
    "            \n",
    "            \n",
    "    #week ahead_data\n",
    "    db_connection = get_sql_connection()\n",
    "    week_ahead_data = week_ahead_df(con=db_connection,\n",
    "                                   site=site,\n",
    "                                   date=ts.date())\n",
    "    week_ahead_data.rename(columns={'swdown':'ghi(w/m2)','temp':'T2'},inplace=True)\n",
    "    week_ahead_data = clearsky_roll_and_shift(clearsky_df=week_ahead_data,\n",
    "                                               target_col='ghi(w/m2)',\n",
    "                                               target_col_new_name='ghi_predicted(w/m2)')\n",
    "    for ts in tqdm(intra_day_df.index):\n",
    "        try:\n",
    "            week_ahead_data.loc[ts,'ghi_predicted(w/m2)'] = intra_day_df.loc[ts,'ghi_predicted(w/m2)']\n",
    "        except:\n",
    "            print(f\"error in replacing week ahead data to nowcasting data\")\n",
    "    \n",
    "#     week_ahead_data.to_csv(os.path.join(intra_day_path,'week_ahead.csv'), index_label='timestamp')\n",
    "    \n",
    "    final_intra_day_df = intra_day_df[['ghi_predicted(w/m2)','site_name']]\n",
    "    try:\n",
    "        last_hour = int(os.path.basename(prev_forecast_file).split('_')[-1].replace('.csv',''))\n",
    "        current_forecast_hour = last_hour+1\n",
    "    except:\n",
    "        current_forecast_hour = 1\n",
    "    print(f\"saving intra day predictions for {site} site\")\n",
    "\n",
    "    os.makedirs(os.path.join(intra_day_path,str(ts.date())), exist_ok = True)\n",
    "    final_intra_day_df.to_csv(os.path.join(intra_day_path,\n",
    "                                         str(ts.date()),\n",
    "                                         str(ts.date())+'_revision_'+str(current_forecast_hour)+'.csv'),\n",
    "                                        index_label='timestamp')\n",
    "    print(f\"writing log file for {site} site\")    \n",
    "    try:    \n",
    "        log_df = pd.read_excel(os.path.join(log_path,str(ts.date())+'_log.xlsx'), sheet_name=site, index_col='timestamp')\n",
    "    except:\n",
    "        log_df = pd.DataFrame()\n",
    "        log_df.index = intra_day_df.index\n",
    "    # if (real_df.shape[0] >0):\n",
    "    #     for i in real_df.index:\n",
    "    #         log_df.loc[i,'real_ghi'] = real_df.loc[i,'ghi(w/m2)']\n",
    "    # else:\n",
    "    #     log_df['real_ghi'] = len(log_df.index)*['no real data']\n",
    "\n",
    "    for i in tqdm(log_df.index):\n",
    "        try:\n",
    "            log_df.loc[i,'cs'] = cs_rolled.loc[i,'cs_rolled']\n",
    "        except:\n",
    "            log_df.loc[i,'cs'] = 'clearsky not available'\n",
    "\n",
    "    for i in tqdm(intra_day_df.index):\n",
    "        ghi_forecast_col = 'ghi_revision_'+str(current_forecast_hour)\n",
    "    #             power_forecast_col = 'intra_day_power_hour_'+str(current_forecast_hour)\n",
    "        forecast_method_col = 'forecast_method_revision_'+str(current_forecast_hour)\n",
    "        try:\n",
    "            log_df.loc[i,ghi_forecast_col] = intra_day_df.loc[i,'ghi_predicted(w/m2)']\n",
    "            log_df.loc[i,forecast_method_col] = intra_day_df.loc[i,'forecast_method']\n",
    "        except:\n",
    "            log_df.loc[i,ghi_forecast_col] = 'nowcasting prediction not available'\n",
    "            log_df.loc[i,forecast_method_col] = 'nowcasting prediction not available'\n",
    "\n",
    "    try:\n",
    "        with pd.ExcelWriter(os.path.join(log_path,\n",
    "                            str(ts.date())+'_log.xlsx'),\n",
    "                            mode=\"a\",\n",
    "                            engine=\"openpyxl\",\n",
    "                            if_sheet_exists='replace') as writer:\n",
    "            log_df.to_excel(writer, sheet_name=site, index_label='timestamp')\n",
    "    except:\n",
    "        with pd.ExcelWriter(os.path.join(log_path,\n",
    "                                     str(ts.date())+'_log.xlsx')) as writer:\n",
    "            log_df.to_excel(writer, sheet_name=site,index_label='timestamp')\n",
    "\n",
    "\n",
    "    extra_cols = [col for col in sample_df_columns if col not in intra_day_df.columns]\n",
    "    for col in extra_cols:\n",
    "        intra_day_df[col] = len(intra_day_df.index)*[np.nan]\n",
    "\n",
    "    intra_day_df1 = intra_day_df[sample_df_columns]\n",
    "\n",
    "    \n",
    "    #delete nowcasting data from database\n",
    "    delete_db_query = f\"DELETE from {db_config.forecast_schema}.{db_config.forecast_table} \" \\\n",
    "                      f\" WHERE timestamp::date = '{str(ts.date())}' and site_name = '{site}' \"\n",
    "    db_connection.execute(delete_db_query)\n",
    "    print(f\"DB intra_day data deleted for {str(ts.date())} date and {site} site \")\n",
    "    #onboarding nowcasting data to database\n",
    "    intra_day_df1.to_sql(con=db_connection,\n",
    "                        schema=db_config.forecast_schema,\n",
    "                        name=db_config.forecast_table, \n",
    "                        if_exists='append',\n",
    "                        index_label='timestamp')\n",
    "    print(f\"Intra day data uploaded to DB for date {str(ts.date())} and {site} site \")\n",
    "    \n",
    "    #delete week ahead api data from database\n",
    "    for dt in tqdm(set(week_ahead_data.index.date)):\n",
    "        delete_db_query = f\"DELETE from {db_config.forecast_schema}.{db_config.api_table} \" \\\n",
    "                          f\" WHERE timestamp::date = '{str(dt)}' and site_name = '{site}' \"\n",
    "        db_connection.execute(delete_db_query)\n",
    "        print(f\"DB week_ahead_api data deleted for {str(dt)} date and {site} site \")\n",
    "    #onboarding week ahead api data to database\n",
    "    week_ahead_data.to_sql(con=db_connection,\n",
    "                            schema=db_config.forecast_schema,\n",
    "                            name=db_config.api_table, \n",
    "                            if_exists='append',\n",
    "                            index_label='timestamp')\n",
    "    print(f\"week_ahead data uploaded to DB for date {str(ts.date())} and {site} site \")\n",
    "\n",
    "print(datetime.now().time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ef3f41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
