{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "169c05fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import glob\n",
    "from datetime import datetime,timedelta,time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from configs import db_config\n",
    "from configs.path_config import *\n",
    "from sqlalchemy import create_engine\n",
    "from pandas import read_sql_query\n",
    "from d6tstack.utils import pd_to_psql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8738c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "977d3b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db_connection(dbname,\n",
    "                         host,\n",
    "                         port,\n",
    "                         user,\n",
    "                         password):\n",
    "    \"\"\"\n",
    "    Establishes connection to database\n",
    "    :param dbname: database name\n",
    "    :param host: endpoint\n",
    "    :param port: port\n",
    "    :param user: username\n",
    "    :param password: password\n",
    "    \"\"\"\n",
    "    connection_string = f'postgresql://{user}:{password}@{host}:{port}/{dbname}'\n",
    "    return create_engine(connection_string)\n",
    "\n",
    "\n",
    "class SiteDataExtractor:\n",
    "\n",
    "    def __init__(self, \n",
    "                 db_connection, \n",
    "                 table_name,\n",
    "                 schema_name,\n",
    "                 site_name,\n",
    "                 today_date,\n",
    "                 site_date_label='timestamp',\n",
    "                 site_column_label='site_name',\n",
    "                 eng='pandas'):\n",
    "        \"\"\"\n",
    "        General purpose class to extract data for a specific site from the schema.table address of the db connection\n",
    "        :param db_connection: sql alchemy db connection\n",
    "        :param table_name: str, table name in the db to extract from\n",
    "        :param schema_name: str, schema name in the db to extract from\n",
    "        :param site_column_label: str, site identifier column label i.e. name of column containing the site names\n",
    "        :param eng: str, ['connectorx', 'pandas']. Defaults to pandas. Can show time improvements in production.\n",
    "        \"\"\"\n",
    "        self.db_connection = db_connection\n",
    "        self.table_name = table_name\n",
    "        self.schema_name = schema_name\n",
    "        self.site_name = site_name\n",
    "        self.today_date = today_date\n",
    "        self.site_column_label = site_column_label\n",
    "        self.site_date_label = site_date_label\n",
    "        self.eng = eng\n",
    "        self.db_str = 'postgresql://admin123:tensor123@tensordb1.cn6gzof6sqbw.us-east-2.rds.amazonaws.com:5432/postgres'\n",
    "\n",
    "    def parse_query(self, query):\n",
    "        \"\"\" parses sql query via the desired engine \"\"\"\n",
    "        if self.eng.lower() == 'pandas':\n",
    "            return pd.read_sql_query(sql=query, con=self.db_connection)\n",
    "        elif self.eng.lower() == 'connectorx':\n",
    "            return cx.read_sql(conn=self.db_str, query=query)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Only 'pandas' and 'connectorx' are valid choices for eng in __init__ call. {self.eng} was provided.\")\n",
    "\n",
    "    def read_data(self):\n",
    "        \"\"\" reads data for a specific site from database \"\"\"\n",
    "        query = f\"select * from {self.schema_name}.{self.table_name}\"                 f\" where {self.site_column_label} = '{self.site_name}' and {self.site_date_label}::date = '{self.today_date}'\"\n",
    "        return self.parse_query(query=query)\n",
    "\n",
    "def utc_to_ist(data_frame, time_col='timestamp'):\n",
    "    temp_data = data_frame.copy()\n",
    "    temp_data[time_col] = pd.to_datetime(temp_data[time_col], utc=True)\n",
    "    return temp_data.set_index(time_col).tz_convert('Asia/Kolkata').reset_index()\n",
    "\n",
    "def remove_timezone(data_frame, time_col='timestamp'):\n",
    "    temp_data = data_frame.copy()\n",
    "    temp_data[time_col] = temp_data[time_col].dt.tz_localize(None)\n",
    "    return temp_data\n",
    "\n",
    "def extract_avail_timeseries_for_variable(data_frame, time_col, variable='ct'):\n",
    "    return data_frame[[time_col, variable]].set_index(time_col)\n",
    "def pre_process_satellite_data(data_frame, time_col='timestamp', variable='ct'):\n",
    "    output = (data_frame\n",
    "              .copy()\n",
    "              .pipe(utc_to_ist, time_col)\n",
    "              .pipe(remove_timezone, time_col)\n",
    "              .pipe(extract_avail_timeseries_for_variable, time_col, variable)\n",
    "              .sort_index())\n",
    "    return output\n",
    "\n",
    "def roundTime(dt, roundTo=15*60):\n",
    "   \"\"\"\n",
    "   Round a datetime object to any time lapse in seconds\n",
    "   dt : datetime.datetime object, default now.\n",
    "   roundTo : Closest number of seconds to round to, default 1 minute.\n",
    "   \"\"\"\n",
    "   if dt != None :\n",
    "        dt = datetime.now()\n",
    "        seconds = (dt.replace(tzinfo=None) - dt.min).seconds\n",
    "        rounding = (seconds+roundTo/2) // roundTo * roundTo\n",
    "        return dt + timedelta(0,rounding-seconds,-dt.microsecond)\n",
    "\n",
    "def day_ahead(db_connection,\n",
    "              table,\n",
    "              schema,\n",
    "              date,\n",
    "              site):\n",
    "    \n",
    "    day_ahead_data = SiteDataExtractor(db_connection=db_connection, \n",
    "                                       table_name=table,\n",
    "                                       schema_name=schema, \n",
    "                                       today_date = str(date),\n",
    "                                       site_name=site).read_data() \n",
    "    if day_ahead_data.shape[0]>0:\n",
    "        \n",
    "    \n",
    "        day_ahead_data['timestamp'] = day_ahead_data['timestamp'].dt.round('15min')\n",
    "        day_ahead_data = day_ahead_data.drop_duplicates(subset=['timestamp','site_name'], keep='last').set_index('timestamp')\n",
    "        day_ahead_data = day_ahead_data[['swdown_wpm2','site_name']]\n",
    "        day_ahead_data = day_ahead_data.rename(columns={'swdown_wpm2':'ghi_predicted(w/m2)'})\n",
    "        print(f\"fetched day_ahead for {site} with {day_ahead_data.shape[0]} rows\")\n",
    "        return day_ahead_data.sort_index()\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    \n",
    "def clearsky(db_connection,\n",
    "             table,\n",
    "             schema,\n",
    "             date,\n",
    "             site,\n",
    "             date_label='times'):\n",
    "    \n",
    "    clearsky_data = SiteDataExtractor(db_connection=db_connection, \n",
    "                                       table_name= table,\n",
    "                                       schema_name=schema,\n",
    "                                       site_date_label=date_label,\n",
    "                                       today_date = str(date),\n",
    "                                       site_name=site).read_data() \n",
    "    if clearsky_data.shape[0]>0:\n",
    "        clearsky_data['times'] = clearsky_data['times'].dt.round('15min')\n",
    "        clearsky_data = clearsky_data.drop_duplicates(subset=['times','site_name'], keep='last').set_index('times')\n",
    "        clearsky_data = clearsky_data[['swdnbc','site_name']]\n",
    "        clearsky_data = clearsky_data.rename(columns={'swdnbc':'cs'})\n",
    "        print(f\"fetched clearsky for {site} with {clearsky_data.shape[0]} rows\")\n",
    "        return clearsky_data.sort_index()\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "def satellite_ct(db_connection,\n",
    "                 table,\n",
    "                 schema,\n",
    "                 timestamp,\n",
    "                 date,\n",
    "                 site,\n",
    "                 satellite_time_col = 'timestamp',\n",
    "                 satellite_ct_col = 'ct'):\n",
    "    \n",
    "    date_utc = (ts-timedelta(hours=5.50)).date()\n",
    "    \n",
    "    satellite_data = SiteDataExtractor(db_connection=db_connection, \n",
    "                                       table_name=table,\n",
    "                                       schema_name=schema, \n",
    "                                       today_date = str(date_utc),\n",
    "                                       site_name=site).read_data()\n",
    "    if (satellite_data.shape[0]>0):\n",
    "        satellite_ct_series = pre_process_satellite_data(data_frame=satellite_data, \n",
    "                                                         time_col= satellite_time_col,\n",
    "                                                         variable=satellite_ct_col)\n",
    "        print(f\"Fetched satellite data for {site} with {satellite_ct_series.shape[0]} rows\")\n",
    "        return satellite_ct_series.sort_index()\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "\n",
    "\n",
    "def ct_post_processing(df,\n",
    "                       site_name,\n",
    "                       sat_forecast_start,\n",
    "                       sat_forecast_end):\n",
    "    \n",
    "    satellite_ct_series = df.copy()\n",
    "    satellite_ct_series = satellite_ct_series.dropna(subset='ct')\n",
    "    satellite_ct_series = satellite_ct_series.reset_index().drop_duplicates(subset = 'timestamp').set_index('timestamp')\n",
    "    \n",
    "    trained_ct_ci = pd.read_csv(os.path.join(resource_path,'trained_ci.csv'))\n",
    "\n",
    "    for i in tqdm(satellite_ct_series.index):\n",
    "        req_ct = satellite_ct_series.loc[i,'ct']\n",
    "        trained_ci = trained_ct_ci[trained_ct_ci['CT_Index']==req_ct]['new_ci_1']\n",
    "        satellite_ct_series.loc[i,'trained_ci'] = trained_ci.item()\n",
    "\n",
    "        req_cs = satellite_ct_series.loc[i,'cs_ghi']\n",
    "        req_ci = satellite_ct_series.loc[i,'trained_ci']\n",
    "        trained_ghi = (1-req_ci)*req_cs\n",
    "        satellite_ct_series.loc[i,'trained_ghi'] = min(trained_ghi,req_cs)\n",
    "    \n",
    "    temp_df = satellite_ct_series.sort_index().copy()\n",
    "    mean_ci_value = temp_df[(temp_df.index.time>=sat_forecast_start) & (temp_df.index.time<=sat_forecast_end)]['trained_ci'].mean()\n",
    "    print(f\"mean_ci_value is {mean_ci_value}\")\n",
    "    if (mean_ci_value>0.1):\n",
    "        start = time(7,0)\n",
    "        end = time(17,0)\n",
    "    #     temp_df['time'] = temp_df.index.time\n",
    "        req_temp_df = temp_df[(temp_df.index.time>=start) & (temp_df.index.time<=end)]\n",
    "    #         print(req_temp_df.shape)\n",
    "        rolling1 = req_temp_df['trained_ghi'].rolling(window=4)\n",
    "    #     rolling2 = site_df['trained_power'].rolling(window=4)\n",
    "        req_temp_df['rolling_mean_trained_ghi'] = rolling1.mean()\n",
    "    #         print(req_temp_df.shape)\n",
    "        req_temp_df = req_temp_df.dropna(subset=['rolling_mean_trained_ghi'])\n",
    "    #         print(req_temp_df.shape)\n",
    "        for idx in req_temp_df.index:\n",
    "            temp_df.loc[idx,'rolling_mean_trained_ghi'] = req_temp_df.loc[idx,'rolling_mean_trained_ghi']\n",
    "\n",
    "        missing_idex = [i for i in temp_df.index if i not in req_temp_df.index]\n",
    "    #     print(len(missing_idex))\n",
    "        for i in missing_idex:\n",
    "            temp_df.loc[i,'rolling_mean_trained_ghi'] = temp_df.loc[i,'trained_ghi']\n",
    "\n",
    "        temp_df = temp_df.rename(columns={'rolling_mean_trained_ghi':'ghi_predicted(w/m2)'})\n",
    "    else:\n",
    "        temp_df = temp_df.rename(columns={'trained_ghi':'ghi_predicted(w/m2)'})\n",
    "#     temp_df = temp_df[['ghi_predicted(w/m2)']]\n",
    "    temp_df['forecast_method'] = len(temp_df.index)*['sat_forecast']\n",
    "    temp_df['site_name'] = len(temp_df.index)*[site_name]\n",
    "    \n",
    "#     temp_df = wrf_ghi_to_power(time_series=temp_df,\n",
    "#                                rad_col='ghi_predicted(w/m2)',\n",
    "#                                power_col='power_predicted(kw)',\n",
    "#                                site_name=site_name,\n",
    "#                                sunrise = '06:00:00',\n",
    "#                                sunset = '18:45:00')\n",
    "    sat_final_df = slice_forecast(df = temp_df,\n",
    "                                  start=sat_forecast_start,\n",
    "                                  end = sat_forecast_end) \n",
    "    return sat_final_df\n",
    "\n",
    "\n",
    "def calculate_prev_timestamp_error(db_connection,\n",
    "                                   real_df,\n",
    "                                   date,\n",
    "                                   site_name,\n",
    "                                   error_smooth_start,\n",
    "                                   error_smooth_end,\n",
    "                                   sat_forecast_start,\n",
    "                                   sat_forecast_end,\n",
    "                                   prev_forecast_df):\n",
    "    \n",
    "   \n",
    "            \n",
    "    error_smooth_df = slice_forecast(df = real_df,\n",
    "                                 start=error_smooth_start,\n",
    "                                 end = error_smooth_end)\n",
    "    if (error_smooth_df.shape[0]>0):\n",
    "        print(f\"Real data available for {site_name} site : calculating error in previous 5 timeblocks\")\n",
    "\n",
    "        for i in error_smooth_df.index:\n",
    "            error_smooth_df.loc[i,'ghi_predicted(w/m2)'] = prev_forecast_df.loc[i,'ghi_predicted(w/m2)']\n",
    "#             error_smooth_df.loc[i,'power_predicted(kw)'] = prev_forecast_df.loc[i,'power_predicted(kw)']\n",
    "\n",
    "        error_smooth_df['error_ghi'] = error_smooth_df['ghi(w/m2)']-error_smooth_df['ghi_predicted(w/m2)']\n",
    "#         error_smooth_df['error_power'] = error_smooth_df['power(kw)']-error_smooth_df['power_predicted(kw)']\n",
    "        ghi_error_sum = error_smooth_df['error_ghi'].mean()\n",
    "#         power_error_sum = error_smooth_df['error_power'].mean()\n",
    "\n",
    "        temp_sat = prev_forecast_df.copy()\n",
    "        temp_sat['ghi_predicted(w/m2)'] = temp_sat['ghi_predicted(w/m2)']+ghi_error_sum\n",
    "#         temp_sat['power_predicted(kw)'] = temp_sat['power_predicted(kw)']+power_error_sum\n",
    "        sat_forecast = slice_forecast(df = temp_sat,\n",
    "                                      start=sat_forecast_start,\n",
    "                                      end = sat_forecast_end)\n",
    "        sat_forecast['forecast_method'] = len(sat_forecast.index)*['error_smooth']\n",
    "        return sat_forecast,error_smooth_df\n",
    "    else:\n",
    "        print(f\"no real data available for {site_name}\")\n",
    "        return pd.DataFrame(),error_smooth_df\n",
    "\n",
    "\n",
    "def slice_forecast(df,start,end):\n",
    "    temp = df.copy()\n",
    "    try:\n",
    "        temp = temp.set_index('timestamp')\n",
    "    except:\n",
    "        pass\n",
    "    temp.index = pd.to_datetime(temp.index)\n",
    "    temp['time'] = temp.index.time\n",
    "    \n",
    "    temp = temp[(temp['time']>=start) &\n",
    "                        (temp['time']<=end)]\n",
    "    return temp\n",
    "\n",
    "def real_data(db_connection,\n",
    "              table,\n",
    "              schema,\n",
    "              date,\n",
    "              site,\n",
    "              time_col = 'timestamp',\n",
    "              rad_col = 'ghi(w/m2)',\n",
    "              power_col = 'power(kw)'):\n",
    "    \n",
    "    real_data = SiteDataExtractor(db_connection=db_connection, \n",
    "                                       table_name= table,\n",
    "                                       schema_name= schema, \n",
    "                                       today_date = str(date),\n",
    "                                       site_name=site).read_data()\n",
    "    if (real_data.shape[0]>0):\n",
    "        real_data = real_data[[time_col,rad_col]].set_index(time_col)\n",
    "        real_data.index = pd.to_datetime(real_data.index)\n",
    "        real_data['time'] = real_data.index.time\n",
    "        print(f\"real data is fetched for {site} with {real_data.shape[0]} rows\")\n",
    "        return real_data\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d8d607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_connection = create_db_connection(dbname=db_config.dbname,\n",
    "                                     host=db_config.host,\n",
    "                                     port = db_config.port,\n",
    "                                     user = db_config.user,\n",
    "                                     password=db_config.password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32c05cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Engine(postgresql://admin123:***@tensordb1.cn6gzof6sqbw.us-east-2.rds.amazonaws.com:5432/postgres)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5645a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intra day script current time is 2023-03-23 14:15:00 \n"
     ]
    }
   ],
   "source": [
    "ts = roundTime(datetime.now(),roundTo=15*60)\n",
    "# ts = datetime(2022,12,1,6)\n",
    "print(f\"intra day script current time is {str(ts)} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fa7b400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched day_ahead for SPP1 with 96 rows\n"
     ]
    }
   ],
   "source": [
    "day_ahead_df = day_ahead(db_connection=db_connection,\n",
    "                          schema=db_config.wrf_schema,\n",
    "                          table = db_config.wrf_view,\n",
    "                          site='SPP1',\n",
    "                          date = ts.date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e1b07a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched clearsky for SPP1 with 96 rows\n"
     ]
    }
   ],
   "source": [
    "clearsky_df = clearsky(db_connection=db_connection,table=db_config.site_clearsky_table, schema=db_config.site_actual_schema,\n",
    "                      date= ts.date(),site='SPP1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aeef6366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cs</th>\n",
       "      <th>site_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>times</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-03-23 08:00:00</th>\n",
       "      <td>294.153630</td>\n",
       "      <td>SPP1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-23 07:45:00</th>\n",
       "      <td>294.153630</td>\n",
       "      <td>SPP1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-23 07:30:00</th>\n",
       "      <td>165.344560</td>\n",
       "      <td>SPP1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-23 07:15:00</th>\n",
       "      <td>165.344560</td>\n",
       "      <td>SPP1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-23 07:00:00</th>\n",
       "      <td>50.774178</td>\n",
       "      <td>SPP1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-23 22:00:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>SPP1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-23 21:45:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>SPP1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-23 21:30:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>SPP1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-23 21:15:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>SPP1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-23 14:45:00</th>\n",
       "      <td>825.673100</td>\n",
       "      <td>SPP1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             cs site_name\n",
       "times                                    \n",
       "2023-03-23 08:00:00  294.153630      SPP1\n",
       "2023-03-23 07:45:00  294.153630      SPP1\n",
       "2023-03-23 07:30:00  165.344560      SPP1\n",
       "2023-03-23 07:15:00  165.344560      SPP1\n",
       "2023-03-23 07:00:00   50.774178      SPP1\n",
       "...                         ...       ...\n",
       "2023-03-23 22:00:00    0.000000      SPP1\n",
       "2023-03-23 21:45:00    0.000000      SPP1\n",
       "2023-03-23 21:30:00    0.000000      SPP1\n",
       "2023-03-23 21:15:00    0.000000      SPP1\n",
       "2023-03-23 14:45:00  825.673100      SPP1\n",
       "\n",
       "[96 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "satellite_df = satellite_ct(db_connection=db_connection,\n",
    "                           table = db_config.satellite_ct_exim_ip_view,\n",
    "                           schema=db_config.satellite_schema,\n",
    "                           timestamp=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ebf079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16946d90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d45488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6693d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intra day script current time is 2022-12-01 06:00:00 \n",
      "Fetched satellite data for Hisar with 30 rows\n",
      "Fetched clearsky data for Hisar with 96 rows\n",
      "satellite data is available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 1143.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_ci_value is 0.32727272727272727\n",
      "no real data available for Hisar\n",
      "saving intra day predictions for Hisar site\n",
      "writing log file for Hisar site\n",
      "DB intra_day data deleted for 2022-12-01 date and Hisar site \n",
      "Intra day data uploaded to DB for date 2022-12-01 and Hisar site \n",
      "sftp connection succesfull\n",
      "files not found\n",
      "code run time for Hisar site is 13.829437255859375 seconds\n",
      "Fetched satellite data for Rewa with 30 rows\n",
      "Fetched clearsky data for Rewa with 96 rows\n",
      "satellite data is available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 1479.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_ci_value is 0.03636363636363637\n",
      "no real data available for Rewa\n",
      "saving intra day predictions for Rewa site\n",
      "writing log file for Rewa site\n",
      "DB intra_day data deleted for 2022-12-01 date and Rewa site \n",
      "Intra day data uploaded to DB for date 2022-12-01 and Rewa site \n",
      "sftp connection succesfull\n",
      "files not found\n",
      "code run time for Rewa site is 14.074566841125488 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "f_date = ts.date()\n",
    "f_time = ts.time()\n",
    "\n",
    "prev_forecast_start = time(0,0)\n",
    "prev_forecast_end = (ts+timedelta(hours=0.25)).time()\n",
    "\n",
    "sat_forecast_start = (ts+timedelta(hours =0.5)).time()\n",
    "sat_forecast_end = (ts+timedelta(hours=3)).time()\n",
    "\n",
    "day_ahead_forecast_start = (ts+timedelta(hours=3.25)).time()\n",
    "day_ahead_forecast_end = time(23,45)\n",
    "\n",
    "error_smooth_start = (ts-timedelta(hours=1.25)).time()\n",
    "error_smooth_end =  (ts-timedelta(hours=0.25)).time()\n",
    "\n",
    "# day_ahead_df = day_ahead(db_connection=db_connection,\n",
    "#                          wrf_table = db_config.wrf_view,\n",
    "#                          wrf_schema = db_config.wrf_schema,\n",
    "#                          date = str(ts.date()),\n",
    "#                          site='Ichhawar')\n",
    "db_connection = create_db_connection(dbname='postgres',\n",
    "                                     host='tensordb1.cn6gzof6sqbw.us-east-2.rds.amazonaws.com',\n",
    "                                     port=5432,\n",
    "                                     user='admin123',\n",
    "                                     password='tensor123')\n",
    "# sample_db_query = f\"select * from {'forecast'}.{'intra_day'} limit 1\"\n",
    "# sample_df = pd.read_sql_query(sample_db_query,con=db_connection, index_col='timestamp')\n",
    "for site in athena_sites:\n",
    "    code_start_time = ti.time()\n",
    "    intra_day_path = os.path.join(home,'Athena',site,'intra_day')\n",
    "   \n",
    "\n",
    "\n",
    "    db_connection = create_db_connection(dbname=db_config.dbname,\n",
    "                                       host=db_config.host,\n",
    "                                       port=db_config.port,\n",
    "                                       user=db_config.user,\n",
    "                                       password=db_config.password)\n",
    "\n",
    "    try:\n",
    "        exim_df,clearsky_df = site_ct_vs_ci_data(db_connection = db_connection,\n",
    "                             satellite_exim_table = db_config.satellite_ct_exim_ip_view,\n",
    "                             satellite_exim_schema= db_config.satellite_schema,\n",
    "                             ts = ts,\n",
    "                             site_name = site,\n",
    "                             clearsky_table=db_config.site_clearsky_table,\n",
    "                             clearsky_schema = db_config.site_actual_schema,\n",
    "                             satellite_ct_col = 'ct')\n",
    "        print('satellite data is available')\n",
    "    except:\n",
    "        exim_df = pd.DataFrame()\n",
    "        clearsky_df = pd.DataFrame()\n",
    "\n",
    "    real_df = real_data_extract(db_connection=db_connection,\n",
    "                                actual_table=db_config.site_actual_table,\n",
    "                                actual_schema =db_config.site_actual_schema,\n",
    "                                date = f_date,\n",
    "                                site= site,\n",
    "                                time_col = 'timestamp',\n",
    "                                rad_col = 'ghi(w/m2)',\n",
    "                                power_col = 'power(kw)')\n",
    "\n",
    "\n",
    "    if (exim_df.shape[0]>0):\n",
    "        exim_forecast_df = ct_post_processing(df=exim_df,\n",
    "                                              site_name= site,\n",
    "                                              sat_forecast_start =sat_forecast_start,\n",
    "                                              sat_forecast_end = sat_forecast_end)\n",
    "    else:\n",
    "        exim_forecast_df = pd.DataFrame()\n",
    "\n",
    "    sat_forecast_df,error_smooth_df = calculate_prev_timestamp_error(db_connection=db_connection,\n",
    "                                                              real_df=real_df,\n",
    "                                                              date = f_date,\n",
    "                                                              site_name= site,\n",
    "                                                              error_smooth_start=error_smooth_start,\n",
    "                                                              error_smooth_end=error_smooth_end,\n",
    "                                                              sat_forecast_start=sat_forecast_start,\n",
    "                                                              sat_forecast_end=sat_forecast_end,\n",
    "                                                              prev_forecast_df=prev_forecast_df)\n",
    "\n",
    "    extra_indexs = [index for index in sat_forecast_df.index if index not in exim_forecast_df.index ]\n",
    "    if (len(extra_indexs)==0):\n",
    "        pass\n",
    "    else:\n",
    "        for idx in extra_indexs:\n",
    "            exim_forecast_df.loc[idx,'ghi_predicted(w/m2)'] = sat_forecast_df.loc[idx,'ghi_predicted(w/m2)']\n",
    "#             exim_forecast_df.loc[idx,'power_predicted(kw)'] = sat_forecast_df.loc[idx,'power_predicted(kw)']\n",
    "            exim_forecast_df.loc[idx,'forecast_method'] = 'error_smooth'\n",
    "            exim_forecast_df['site_name'] = len(exim_forecast_df.index)*[site]\n",
    "\n",
    "            \n",
    "    \n",
    "    \n",
    "    if (exim_forecast_df.shape[0]>0):\n",
    "        intra_day_second_df = exim_forecast_df[cols]\n",
    "\n",
    "        # # Day Ahead Forecast\n",
    "\n",
    "        intra_day_third_df = slice_forecast(df = day_ahead_df,\n",
    "                                            start = day_ahead_forecast_start,\n",
    "                                            end = day_ahead_forecast_end) \n",
    "        intra_day_third_df['forecast_method'] = len(intra_day_third_df.index)*['day_ahead']\n",
    "\n",
    "        intra_day_third_df = intra_day_third_df[cols]\n",
    "\n",
    "        intra_day_df = pd.concat([intra_day_first_df,\n",
    "                                  intra_day_second_df,\n",
    "                                  intra_day_third_df], axis=0)\n",
    "#         if (clearsky_df.shape[0]>0):\n",
    "#             for i in intra_day_df.index:\n",
    "#                 req_cs = clearsky_df.loc[i,'cs_power(kw)']\n",
    "#                 req_power = intra_day_df.loc[i,'power_predicted(kw)'] \n",
    "#                 lower_forecast_limit = req_cs*0.15\n",
    "#                 upper_forecast_limit = req_cs*0.95\n",
    "#                 #print(i,lower_forecast_limit,req_power,upper_forecast_limit)\n",
    "#                 intra_day_df.loc[i,'power_predicted(kw)']  = max(min(upper_forecast_limit,req_power),lower_forecast_limit)\n",
    "\n",
    "        #intra_day_df['power_predicted(kw)'] = intra_day_df['power_predicted(kw)'].clip(lower=0)\n",
    "        intra_day_df['ghi_predicted(w/m2)'] = intra_day_df['ghi_predicted(w/m2)'].clip(lower=0)\n",
    "        intra_day_df = fill_nan_with_prev_next_mean(df=intra_day_df,\n",
    "                                                    col = 'ghi_predicted(w/m2)')\n",
    "        \n",
    "        for i in intra_day_df.index:\n",
    "            if np.isnan(intra_day_df.loc[i,'ghi_predicted(w/m2)']):\n",
    "                intra_day_df.loc[i,'ghi_predicted(w/m2)'] = clearsky_df.loc[i,'cs_ghi']\n",
    "            if np.isnan(intra_day_df.loc[i,'ghi_predicted(w/m2)']):\n",
    "                intra_day_df.loc[i,'ghi_predicted(w/m2)'] = day_ahead_df.loc[i,'ghi_predicted(w/m2)']\n",
    "            \n",
    "#         intra_day_power_df = intra_day_df[['power_predicted(kw)','site_name']]\n",
    "        intra_day_rad_df = intra_day_df[['ghi_predicted(w/m2)','site_name']]\n",
    "        try:\n",
    "            \n",
    "            last_hour = int(os.path.basename(prev_forecast_file).split('_')[-1].replace('.csv',''))\n",
    "            current_forecast_hour = last_hour+1\n",
    "        except:\n",
    "            current_forecast_hour = 1\n",
    "\n",
    "        print(f\"saving intra day predictions for {site} site\")\n",
    "\n",
    "        os.makedirs(os.path.join(intra_day_path,'radiation',str(f_date)), exist_ok = True)\n",
    "        intra_day_rad_df.to_csv(os.path.join(intra_day_path,\n",
    "                                             'radiation',\n",
    "                                             str(f_date),\n",
    "                                             site +'_'+str(f_date)+'_intra_day_hour_'+str(current_forecast_hour)+'.csv'),\n",
    "                                            index_label='timestamp')\n",
    "\n",
    "        print(f\"writing log file for {site} site\")\n",
    "        \n",
    "        try:    \n",
    "            log_df = pd.read_excel(os.path.join(log_path,str(f_date)+'_log.xlsx'), sheet_name=site, index_col='timestamp')\n",
    "        except:\n",
    "            log_df = pd.DataFrame()\n",
    "            log_df.index = intra_day_df.index\n",
    "        if (real_df.shape[0] >0):\n",
    "            for i in real_df.index:\n",
    "                log_df.loc[i,'real_ghi'] = real_df.loc[i,'ghi(w/m2)']\n",
    "        else:\n",
    "            log_df['real_ghi'] = len(log_df.index)*['no real data']\n",
    "#             log_df.loc[i,'real_power'] = real_df.loc[i,'power(kw)']\n",
    "\n",
    "        for i in clearsky_df.index:\n",
    "            log_df.loc[i,'cs_ghi'] = clearsky_df.loc[i,'cs_ghi']\n",
    "#             log_df.loc[i,'cs_power'] = clearsky_df.loc[i,'cs_power(kw)']\n",
    "\n",
    "        for i in intra_day_df.index:\n",
    "            ghi_forecast_col = 'intra_day_ghi_hour_'+str(current_forecast_hour)\n",
    "#             power_forecast_col = 'intra_day_power_hour_'+str(current_forecast_hour)\n",
    "            forecast_method_col = 'forecast_method_hour_'+str(current_forecast_hour)\n",
    "            log_df.loc[i,ghi_forecast_col] = intra_day_df.loc[i,'ghi_predicted(w/m2)']\n",
    "#             log_df.loc[i,power_forecast_col] = intra_day_df.loc[i,'power_predicted(kw)']\n",
    "            log_df.loc[i,forecast_method_col] = intra_day_df.loc[i,'forecast_method']\n",
    "\n",
    "        try:\n",
    "            with pd.ExcelWriter(os.path.join(log_path,\n",
    "                                str(f_date)+'_log.xlsx'),\n",
    "                                mode=\"a\",\n",
    "                                engine=\"openpyxl\",\n",
    "                                if_sheet_exists='replace') as writer:\n",
    "                log_df.to_excel(writer, sheet_name=site)\n",
    "        except:\n",
    "            with pd.ExcelWriter(os.path.join(log_path,\n",
    "                                         str(f_date)+'_log.xlsx')) as writer:\n",
    "                log_df.to_excel(writer, sheet_name=site)\n",
    "        \n",
    "#         with pd.ExcelWriter(os.path.join(log_path,\n",
    "#                                          str(f_date)+'_log.xlsx'),\n",
    "#                                          mode=\"a\",\n",
    "#                                          engine=\"openpyxl\",\n",
    "#                                          if_sheet_exists='replace') as writer:\n",
    "\n",
    "#             log_df.to_excel(writer, sheet_name=site,)\n",
    "\n",
    "\n",
    "      #---------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "       #delete current date data from database\n",
    "        delete_db_query = f\"DELETE from {'forecast'}.{'intra_day'} \" \\\n",
    "                          f\" WHERE timestamp::date = '{str(f_date)}' and site_name = '{site}' \"\n",
    "        db_connection.execute(delete_db_query)\n",
    "        print(f\"DB intra_day data deleted for {str(f_date)} date and {site} site \")\n",
    "        #onboarding day ahead data to database\n",
    "        extra_cols = [col for col in sample_df.columns if col not in intra_day_df.columns]\n",
    "        for col in extra_cols:\n",
    "            intra_day_df[col] = len(intra_day_df.index)*[np.nan]\n",
    "        final_intra_day_df = intra_day_df[sample_df.columns]\n",
    "        final_intra_day_df.to_sql(con=db_connection,\n",
    "                                    schema='forecast',\n",
    "                                    name='intra_day', \n",
    "                                    if_exists='append',\n",
    "                                    index_label='timestamp')\n",
    "        print(f\"Intra day data uploaded to DB for date {str(f_date)} and {site} site \")\n",
    "\n",
    "        #copy to sftp server\n",
    "        #power file\n",
    "        file_name =  site +'_hour_'+str(current_forecast_hour)+'.csv'\n",
    "        sftp_rad_intra_day_path = os.path.join('output','weather','Intra_day',site,str(f_date))\n",
    "        #radiation file\n",
    "        file_name =  site +'_hour_'+str(current_forecast_hour)+'.csv'\n",
    "        \n",
    "        sftp_copy(file_name=file_name,\n",
    "                   site_name=site,\n",
    "                   date = f_date,\n",
    "                   local_file_path = os.path.join(intra_day_path,\n",
    "                                                  'radiation',\n",
    "                                                   str(f_date),\n",
    "                                                   site +'_'+str(f_date)+'_intra_day_hour_'+str(current_forecast_hour)+'.csv'),\n",
    "                   sftp_remote_path = sftp_rad_intra_day_path,\n",
    "                   host = '54.161.218.249',\n",
    "                   username='athena',\n",
    "                   password='ATHENA@tensor2022')\n",
    "    else:\n",
    "        print(f\"satellite forecast or real data not available for {site} site : please check\")\n",
    "        pass\n",
    "      #--------------------------------------------------------------------------------------------------------------  \n",
    "    code_end_time = ti.time()\n",
    "    code_run_time = code_end_time - code_start_time\n",
    "    print(f\"code run time for {site} site is {code_run_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a66b17d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pysftp\n",
      "  Using cached pysftp-0.2.9-py3-none-any.whl\n",
      "Requirement already satisfied: paramiko>=1.17 in c:\\users\\jagjeet\\anaconda3\\lib\\site-packages (from pysftp) (2.8.1)\n",
      "Requirement already satisfied: cryptography>=2.5 in c:\\users\\jagjeet\\anaconda3\\lib\\site-packages (from paramiko>=1.17->pysftp) (3.4.8)\n",
      "Requirement already satisfied: pynacl>=1.0.1 in c:\\users\\jagjeet\\anaconda3\\lib\\site-packages (from paramiko>=1.17->pysftp) (1.4.0)\n",
      "Requirement already satisfied: bcrypt>=3.1.3 in c:\\users\\jagjeet\\anaconda3\\lib\\site-packages (from paramiko>=1.17->pysftp) (3.2.0)\n",
      "Requirement already satisfied: cffi>=1.1 in c:\\users\\jagjeet\\anaconda3\\lib\\site-packages (from bcrypt>=3.1.3->paramiko>=1.17->pysftp) (1.15.0)\n",
      "Requirement already satisfied: six>=1.4.1 in c:\\users\\jagjeet\\anaconda3\\lib\\site-packages (from bcrypt>=3.1.3->paramiko>=1.17->pysftp) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\jagjeet\\anaconda3\\lib\\site-packages (from cffi>=1.1->bcrypt>=3.1.3->paramiko>=1.17->pysftp) (2.21)\n",
      "Installing collected packages: pysftp\n",
      "Successfully installed pysftp-0.2.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pysftp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "050511b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def site_ct_vs_ci_data(db_connection,\n",
    "                       satellite_exim_table,\n",
    "                       satellite_exim_schema,\n",
    "                       ts,\n",
    "                       site_name,\n",
    "                       clearsky_table,\n",
    "                       clearsky_schema,\n",
    "                       satellite_time_col = 'timestamp',\n",
    "                       satellite_ct_col = 'ct'):\n",
    "    date_utc = (ts-timedelta(hours=5.50)).date()\n",
    "    \n",
    "    satellite_data = SiteDataExtractor(db_connection=db_connection, \n",
    "                                   table_name=satellite_exim_table,\n",
    "                                   schema_name=satellite_exim_schema, \n",
    "                                   today_date = str(date_utc),\n",
    "                                   site_name=site_name).read_data()\n",
    "#     print(f\"Fetched satellite data for {site_name} with {satellite_data.shape[0]} rows\")\n",
    "    satellite_ct_series = pre_process_satellite_data(data_frame=satellite_data, \n",
    "                                                     time_col='timestamp',\n",
    "                                                     variable='ct')\n",
    "    print(f\"Fetched satellite data for {site_name} with {satellite_ct_series.shape[0]} rows\")\n",
    "    return satellite_ct_series\n",
    "\n",
    "def ct_post_processing(df,\n",
    "                       site_name,\n",
    "                       sat_forecast_start,\n",
    "                       sat_forecast_end):\n",
    "    \n",
    "    satellite_ct_series = df.copy()\n",
    "    satellite_ct_series = satellite_ct_series.dropna(subset='ct')\n",
    "    satellite_ct_series = satellite_ct_series.reset_index().drop_duplicates(subset = 'timestamp').set_index('timestamp')\n",
    "    \n",
    "    trained_ct_ci = pd.read_csv(os.path.join(resource_path,'trained_ci.csv'))\n",
    "\n",
    "    for i in tqdm(satellite_ct_series.index):\n",
    "        req_ct = satellite_ct_series.loc[i,'ct']\n",
    "        trained_ci = trained_ct_ci[trained_ct_ci['CT_Index']==req_ct]['new_ci_1']\n",
    "        satellite_ct_series.loc[i,'trained_ci'] = trained_ci.item()\n",
    "\n",
    "    return satellite_ct_series"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
